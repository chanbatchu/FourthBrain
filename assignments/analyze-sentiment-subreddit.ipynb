{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit and HuggingFace Starter Kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( ü§ó the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- At the end of each task, commit* the work into your own remote repo\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your remote repo\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 0.  Get updates from `FourthBrain/MLE-7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Under your forked local repo, **fetch** and download new updates from repo `FourthBrain/MLE-7` locally so you can start development. \n",
    "    \n",
    "<details>\n",
    "<summary>If you haven't added `FourthBrain/MLE-7` as a remote repo, click here for instructions:</summary>   \n",
    "You fork repo `FourthBrain/MLE-7` to `yourhandle/MLE-7`, clone it locally, and now you are under directory `MLE-7`. By default, you will see one server name `origin` pointing to your repo:  \n",
    "    \n",
    "```\n",
    "$git remote -v \n",
    "origin  git@github.com:yourhandle/MLE-7.git (fetch)\n",
    "origin  git@github.com:yourhandle/MLE-7.git (push)\n",
    "```\n",
    "\n",
    "Think of fetch = read and push = write. \n",
    "\n",
    "Now add `FourthBrain/MLE-7` as a remote repo\n",
    "\n",
    "```\n",
    "$git add remote fourthbrain git@github.com:FourthBrain/MLE-7.git\n",
    "$git remote -v\n",
    "fourthbrain\tgit@github.com:FourthBrain/MLE-7.git (fetch)\n",
    "fourthbrain\tgit@github.com:FourthBrain/MLE-7.git (push)\n",
    "origin git@github.com:yourhandle/MLE-7.git (fetch)\n",
    "origin git@github.com:yourhandle/MLE-7.git (push)\n",
    "```\n",
    "\n",
    "then before each session starts, run `git fetch fourthbrain` to get updates (why not `git pull`?).\n",
    "\n",
    "check out [Working with Remotes](https://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes) for more explanations.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "conda activate {fourthbrain}\n",
    "pip install -U transformers praw torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate fourthbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (4.19.0)\n",
      "Requirement already satisfied: praw in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (7.6.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (1.22.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from praw) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/envs/fourthbrain/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers praw torch numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect url** ( The redirect URI is where the user is sent after they've granted OAuth access to your application; more info [here](and details are in [](https://github.com/reddit-archive/reddit/wiki/OAuth2) for our purpose, you can enter some random url, e.g., www.google.com; ) as shown below.\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jolt down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = {}\n",
    "    REDDIT_API_CLIENT_SECRET = {}\n",
    "    REDDIT_API_USER_AGENT = {}\n",
    "    ```\n",
    "- Add `secrets.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import secrets\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id = secrets.REDDIT_API_CLIENT_ID,\n",
    "    client_secret = secrets.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent = secrets.REDDIT_API_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x7ffdc0a36dc0>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected output you will see ar from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "subreddit = reddit.subreddit('wallstreetbets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallstreetbets\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallstreetbets\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our official Twitter: [@Official_WSB](https://twitter.com/official_wsb)\n",
      "\n",
      "The Bank of Japan has not responded to our questions regarding why Japan's yield curve looks like Japan: http://www.econ.yale.edu/smith/econ116a/japan.pdf\n",
      "\n",
      "---\n",
      "\n",
      "The rules and submission guidelines are maintained on new Reddit so be sure to check them and make sure you're up to date.\n",
      "\n",
      "* Read the [rules](https://www.reddit.com/r/wallstreetbets/about/rules)\n",
      "\n",
      "* Read the [comment and submission guide](https://www.reddit.com/r/wa\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.description[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtime_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mgenerator_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a :class:`.ListingGenerator` for top items.\n",
      "\n",
      ":param time_filter: Can be one of: ``\"all\"``, ``\"day\"``, ``\"hour\"``,\n",
      "    ``\"month\"``, ``\"week\"``, or ``\"year\"`` (default: ``\"all\"``).\n",
      "\n",
      ":raises: :py:class:`ValueError` if ``time_filter`` is invalid.\n",
      "\n",
      "Additional keyword arguments are passed in the initialization of\n",
      ":class:`.ListingGenerator`.\n",
      "\n",
      "This method can be used like:\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "    reddit.domain(\"imgur.com\").top(time_filter=\"week\")\n",
      "    reddit.multireddit(redditor=\"samuraisam\", name=\"programming\").top(time_filter=\"day\")\n",
      "    reddit.redditor(\"spez\").top(time_filter=\"month\")\n",
      "    reddit.redditor(\"spez\").comments.top(time_filter=\"year\")\n",
      "    reddit.redditor(\"spez\").submissions.top(time_filter=\"all\")\n",
      "    reddit.subreddit(\"all\").top(time_filter=\"hour\")\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/envs/fourthbrain/lib/python3.8/site-packages/praw/models/listing/mixins/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "?subreddit.top  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times Square right now\n",
      "UPVOTE so everyone sees we got SUPPORT\n",
      "GME YOLO update ‚Äî Jan 28 2021\n",
      "GME YOLO month-end update ‚Äî Jan 2021\n",
      "It‚Äôs treason then\n",
      "CLASS ACTION AGAINST ROBINHOOD. Allowing people to only sell is the definition of market manipulation. A class action must be started, Robinhood has made plenty of money off selling info about our trades to the hedge funds to be able to pay out a little for causing people to loose money now\n",
      "GME YOLO update ‚Äî Feb 19 2021\n",
      "GME YOLO update ‚Äî Jan 27 2021 --------------------------------------- guess i need 102 characters in title now\n",
      "GME YOLO update ‚Äî Feb 1 2021\n",
      "Used some of my GME tendies to buy Nintendo Switches from Gamestop, then donated them to a Children's Hospital. Got featured on the local news and brought glory to WSB.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for post in subreddit.top(limit = 10):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'Big Short 2' is gonna be lit\n",
      "This week..!\n",
      "Everyone this morning:\n",
      "In and out Morty, a quick 24h adventure! üî•\n",
      "Market Cycles\n",
      "üí©üôå\n",
      "How can fund managers contend with the disruptive new technology coming to their industry?\n",
      "Fun fact\n",
      "Just a reminder that it‚Äôs just money\n",
      "Just inflation.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for post in subreddit.top(time_filter=\"week\",limit = 10):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple‚Äôs director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said ‚ÄúI believe strongly that more flexibility would have been the best policy for my team.‚Äù He was likely the company‚Äôs most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I‚Äôve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple‚Äôs version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later üòä) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 121 ms, total: 1.3 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# creating a list of comments - initiate\n",
    "top_comments = []\n",
    "\n",
    "# for each submission in top 10 submissions\n",
    "for submission in subreddit.top(limit=10):\n",
    "    # for each comment in each submission\n",
    "    for top_level_comment in submission.comments:\n",
    "        # check if top_level_comment is more comments object\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # append to the list of top submissions\n",
    "        top_comments.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE  # the answer may vary 693 for r/machinelearning\n",
    "len(top_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Him posting this literally moved the stonk after hours by $20. Just spam post your positions every minute pretty please',\n",
       " \"My mom told me while I was setting up an account that if the stock market gets fucked I am too, because most of my inheritance is in stocks so do I reeeeealy wanna take part in this?\\n\\nI now hold 2 shares in GME. Let's go boys.\",\n",
       " 'If you want to _see_ $GME on Robinhood \\n\\nSearch >  ‚Äò100 Most Popular‚Äô > scroll down\\n\\nYou can see $GME $NOK $AMC']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "[random.choice(top_comments) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "subreddit = reddit.subreddit('TSLA')\n",
    "# creating a list of comments - initiate\n",
    "top_comments_tsla = []\n",
    "\n",
    "# for each submission in top 10 submissions\n",
    "for submission in subreddit.top(limit=10,time_filter = 'year'):\n",
    "    # for each comment in each submission\n",
    "    for top_level_comment in submission.comments:\n",
    "        # check if top_level_comment is more comments object\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # append to the list of top submissions\n",
    "        top_comments_tsla.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) # Expected: 174 for r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I bought puts', 'Yes Yes Yes', 'How can these people sleep well at night?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I‚Äôm bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:00<00:00, 170kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255M/255M [01:05<00:00, 4.11MB/s] \n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 10.1kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 226k/226k [00:04<00:00, 49.3kB/s] \n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\")# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Would be great but Elon said he wasn't in a hurry to do that again\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentiment = sentiment_model(\"Would be great but Elon said he wasn't in a hurry to do that again\")# YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9818629622459412}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "YOUR ANSWER HERE:\n",
    "[{'label': 'NEGATIVE', 'score': 0.9818629622459412}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: Would be great but Elon said he wasn't in a hurry to do that again\n",
      "Predicted Label is NEGATIVE and the score is 0.982\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_tlsa_comment_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import secrets\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: [description]\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id=secrets.REDDIT_API_CLIENT_ID,        \n",
    "        client_secret=secrets.REDDIT_API_CLIENT_SECRET,\n",
    "        user_agent=secrets.REDDIT_API_USER_AGENT\n",
    "        )\n",
    "    \n",
    "    subreddit = reddit.subreddit('TSLA')# YOUR CODE HERE\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): [description]\n",
    "        limit (int, optional): [description]. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): [description]\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = pipeline(\"sentiment-analysis\")# YOUR CODE HERE\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    submission = subreddit.top(time_filter=\"year\",limit = 100)# YOUR CODE HERE\n",
    "    comments = get_comments(submission)\n",
    "    comment = comments# YOUR CODE HERE\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"top_tlsa_comment_sentiment.py\", line 65, in <module>\n",
      "    submission = subreddit.top(limit = 10)# YOUR CODE HERE\n",
      "NameError: name 'subreddit' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f6b81284c249c74561232734b087daca8bae8a4fd3e0fb145e6432daaeb6de3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('fourthbrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
